


# 1) Setup & Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
import warnings
warnings.filterwarnings('ignore')
sns.set(style='whitegrid')

print('Libraries imported successfully')



# 2) Load Data
# Make sure 'train.csv' is in the same folder as this notebook.

if not any(os.path.exists(fname) for fname in ['train.csv', 'test.csv']):
    print('WARNING: train.csv and test.csv not found in working directory.\nPlease download the Airline Passenger Satisfaction dataset from Kaggle and place train.csv here.')

# Try loading train.csv
try:
    data = pd.read_csv('train.csv')
    print('Loaded train.csv — shape:', data.shape)
except Exception as e:
    print('Could not load train.csv:', e)
    data = pd.DataFrame()

# Show top rows if loaded
if not data.empty:
    display(data.head())
    display(data.info())






# Quick EDA (run only if data loaded)
if data.empty:
    raise SystemExit('Load the dataset first (train.csv) and re-run this cell.')

# Target distribution
plt.figure(figsize=(6,4))
ax = sns.countplot(x='satisfaction', data=data)
ax.set_title('Satisfaction Distribution')
plt.show()


# Missing values
print('\nMissing values per column:')
print(data.isnull().sum())


# Numeric summary
display(data.describe())


# Correlation heatmap for numeric columns
numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
plt.figure(figsize=(12,8))
sns.heatmap(data[numeric_cols].corr(), annot=False, cmap='coolwarm')
plt.title('Correlation heatmap (numeric features)')
plt.show()





# Required libraries
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns

# Optional smoothing
try:
    from statsmodels.nonparametric.smoothers_lowess import lowess
    HAS_LOWESS = True
except Exception:
    HAS_LOWESS = False

# --- Load data (adjust path or use existing df) ---
df = pd.read_csv('train.csv')  # uncomment & edit if needed
# Assume df has columns: 'Age' and 'ticket_type' (case-sensitive)
# Example: df = pd.DataFrame({'Age':[...], 'ticket_type':[...]})


# --- Preprocessing for plotting ---
# restrict age range to sensible values (adjust if needed)
min_age, max_age = 18, 70
age_range = list(range(min_age, max_age + 1))

# ensure Age is integer
df = df.copy()
df['Age'] = pd.to_numeric(df['Age'], errors='coerce').dropna().astype(int)
df = df[(df['Age'] >= min_age) & (df['Age'] <= max_age)]

# list of ticket types to plot (order preserved)
ticket_types = df['ticket_type'].dropna().unique().tolist()
# If you want a fixed order, define it:
# ticket_types = ['Technical issue','Billing inquiry','Cancellation request','Product inquiry','Refund request']

# --- Figure layout ---
n = len(ticket_types)
cols = 3
rows = int(np.ceil(n / cols))
fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 4), constrained_layout=True)
axes = axes.flatten()

# plotting
for i, t in enumerate(ticket_types):
    ax = axes[i]
    sub = df[df['ticket_type'] == t]
    # count per age (every integer age between min_age and max_age)
    counts = sub['Age'].value_counts().reindex(age_range, fill_value=0).sort_index()
    ages = np.array(counts.index)
    vals = counts.values

    # bar chart
    ax.bar(ages, vals, width=0.8, alpha=0.7)

    # smooth trend (LOWESS preferred, fallback to rolling mean)
    if HAS_LOWESS and len(ages) >= 10:
        smooth = lowess(vals, ages, frac=0.12, return_sorted=False)
        ax.plot(ages, smooth, linewidth=2)
    else:
        smooth = pd.Series(vals, index=ages).rolling(window=7, center=True, min_periods=1).mean()
        ax.plot(ages, smooth, linewidth=2)

    # labels and styling
    ax.set_title(t, fontsize=12)
    ax.set_xlabel('Age', fontsize=10)
    ax.set_ylabel('Number of Tickets', fontsize=10)
    ax.set_xlim(min_age - 1, max_age + 1)
    # optional: set x ticks every 5 years to reduce clutter
    ax.set_xticks(list(range(min_age, max_age + 1, 5)))
    sns.despine(ax=ax)

# clear any unused subplots
for j in range(n, rows * cols):
    fig.delaxes(axes[j])

fig.suptitle('Distribution of Ticket Types by Age', fontsize=16)
plt.show()



plt.figure(figsize=(10, 5))

# Histogram with KDE
sns.histplot(
    data['Age'], 
    bins=20, 
    kde=True, 
    color='salmon', 
    edgecolor='black',
    alpha=0.4
)

plt.title('Customer Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()



# Age groups on x-axis
age_groups = ['0-20', '21-30', '31-40', '41-50',
              '51-60', '61-70', '71-80', '81-90', '91-100']

# Number of tickets for each age group (example values – change as needed)
tickets = [320, 1600, 1550, 1600, 1670, 1580, 150, 0, 0]

plt.figure(figsize=(8, 4))
plt.bar(age_groups, tickets, color='skyblue', edgecolor='black')

plt.title('Tickets Raised by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Number of Tickets Raised')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()






# Preprocessing
# 1) Drop rows with NA in target if any
if data['satisfaction'].isnull().any():
    data = data.dropna(subset=['satisfaction'])


# 2) Simple fill for other missing values (if any)
for col in data.columns:
    if data[col].isnull().sum() > 0:
        if data[col].dtype == 'O':
            data[col].fillna(data[col].mode()[0], inplace=True)
        else:
            data[col].fillna(data[col].median(), inplace=True)


# 3) Encode categorical variables
cat_cols = data.select_dtypes(include=['object']).columns.tolist()
print('Categorical columns:', cat_cols)
le_dict = {}
for col in cat_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col].astype(str))
    le_dict[col] = le


# 4) Prepare features and target
X = data.drop(['satisfaction'], axis=1)
y = data['satisfaction']

print('\nFeature matrix shape:', X.shape)
print('Target shape:', y.shape)


# 5) Optional: Scale numeric features
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
scaler = StandardScaler()
X[num_cols] = scaler.fit_transform(X[num_cols])

# Save encoders and scaler for later (in memory) - will save model & pipeline later
import pickle
with open('label_encoders.pkl', 'wb') as f:
    pickle.dump(le_dict, f)
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

print('Preprocessing complete. Encoders and scaler saved locally.')








# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)
print('Train set:', X_train.shape, 'Test set:', X_test.shape)

# Train Random Forest
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)

# Predict
y_pred = rf.predict(X_test)

# Evaluate
print('Accuracy:', accuracy_score(y_test, y_pred))
print('\nClassification Report:\n', classification_report(y_test, y_pred))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()









# Feature importance
feat_imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
print(feat_imp.head(15))


plt.figure(figsize=(8,6))
feat_imp.head(15).plot(kind='barh')
plt.gca().invert_yaxis()
plt.title('Top 15 Feature Importances')
plt.show()





# Small GridSearch (optional — may take time depending on machine)
# Uncomment to run
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}
grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, n_jobs=-1, scoring='accuracy')
grid.fit(X_train, y_train)
print('Best params:', grid.best_params_)
best_rf = grid.best_estimator_









# Save model and artifacts
joblib.dump(rf, 'rf_airline_satisfaction_model.pkl')
print('Saved Random Forest model to rf_airline_satisfaction_model.pkl')

# Artifacts saved: label_encoders.pkl, scaler.pkl, rf_airline_satisfaction_model.pkl
print('Saved artifacts: label_encoders.pkl, scaler.pkl, rf_airline_satisfaction_model.pkl')




